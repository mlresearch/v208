---
title: Issues for Continual Learning in the Presence of Dataset Bias
abstract: While most continual learning algorithms have focused on tackling the stability-plasticity
  dilemma, they have overlooked the effects of the knowledge transfer when the dataset
  is biased â€” namely, when some unintended spurious correlations, not the true causal
  structures, of the tasks are learned from the biased dataset. In that case, how
  would they affect learning future tasks or the knowledge already learned from the
  past tasks? In this work, we design systematic experiments with a synthetic biased
  dataset and try to answer the above question from our empirical findings. Namely,
  we first show that standard continual learning methods that are unaware of dataset
  bias can transfer biases from one task to another, both forward and backward. In
  addition, we find that naively using existing debiasing methods after each continual
  learning step can lead to significant forgetting of past tasks and reduced overall
  continual learning performance. These findings highlight the need for a causality-aware
  design of continual learning algorithms to prevent both bias transfers and catastrophic
  forgetting.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lee23a
month: 0
tex_title: Issues for Continual Learning in the Presence of Dataset Bias
firstpage: 92
lastpage: 99
page: 92-99
order: 92
cycles: false
bibtex_author: Lee, Donggyu and Jung, Sangwon and Moon, Taesup
author:
- given: Donggyu
  family: Lee
- given: Sangwon
  family: Jung
- given: Taesup
  family: Moon
date: 2023-06-04
address: 
container-title: Proceedings of The First AAAI Bridge Program on Continual Causality
volume: '208'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 6
  - 4
pdf: https://proceedings.mlr.press/v208/lee23a/lee23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
